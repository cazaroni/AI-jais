# AI - Jan 27th

Created: January 28, 2025 9:32 AM
Class: AI

I haven’t been to at least two weeks worth of lectures, so where do I go from here? Reading the past few presentations and catching up to speed, of course.

# Artificial Intelligence - Decision Tree

The concept of binary search trees has been teased since my Teoria de la Computacion class (which is probably translated to Theory of Computation since Computational Theory apparently leads to another topic every time I google it) and we had programmed a version of it in our Data Structure class. At the time it didn’t seem like there was a whole lot going on for it other than unnecessarily long and bloated algorithms, but it did open the path for exploring back-tracing.

Decision trees seems to be expanding upon this idea and implements new concepts to digest. As I’m still trying to catch up with everything I’ve missed out so far, I’ll just scribble down what I was able to understand from both the lecture and the PowerPoint slides.

## Decisions to make

Typically demonstrated with this concept is recursion (and tons of it). Let’s say we needed to find the cheapest way to get from Point A to point J, the greedy algorithm is a shining example of how to begin employing recursion. Of course, establishing a base case is essential to be able to properly adapt this algorithm for our purposes.

On the more mathematical side of things, we assign attributes to the variables found on decision trees, alongside classes that we can assign to the “leaf”. These two form together to create hypotheses for the tree and present the possible inputs and outputs of a function.

## Entropies and Antelopes

In decision trees, entropy is a measure of impurity or disorder in a dataset. It quantifies how mixed the classes are at any given node. The formula for entropy is:

H(S) = -Σ p(i) log₂ p(i)

where p(i) is the proportion of class i in the dataset. A perfectly pure node (all samples belong to the same class) has entropy = 0, while maximum impurity (equal distribution of classes) has the highest entropy.

This concept is crucial for decision tree algorithms as it helps determine the best attributes for splitting nodes. The goal is to reduce entropy (increase information gain) with each split, leading to more organized and meaningful classifications.

(I used AI to generate the summary above me! Just wanted to see if the Notion AI was good at simplifying my long notes or not.)